{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is for sampling sequence of words.\n",
    "To run more than once, restart the kernel.\n",
    "\n",
    "#### 1. Import Libraries\n",
    "The first cell imports libraries. This code uses Tensorflow version 0.9. If Tensorflow version is not the same, the code may not work as expected.\n",
    "Make sure Tensorflow environment is on before starting this notebook.\n",
    "\n",
    "```\n",
    "> source activate tensorflow\n",
    "```\n",
    "\n",
    "Some libraries need to be install after Tensorflow enviroment starts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from six.moves import cPickle\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import rnn\n",
    "\n",
    "logging = tf.logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Model definition\n",
    "This is a model definition. The model uses LSTM (long short term memory) cell, which is stacked up by config.num_layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    \"\"\"Model definition\"\"\"\n",
    "\n",
    "    def __init__(self, is_training, config):\n",
    "        self.batch_size = batch_size = config.batch_size\n",
    "        self.num_steps = num_steps = config.num_steps\n",
    "        hidden_size = config.hidden_size\n",
    "        vocab_size = config.vocab_size\n",
    "\n",
    "        self._input_data = tf.placeholder(tf.int32, [batch_size, num_steps])\n",
    "        self._targets = tf.placeholder(tf.int32, [batch_size, num_steps])\n",
    "\n",
    "        # gets LSTM cell\n",
    "        lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_size, forget_bias=config.forget_bias)\n",
    "        # dropout setting, increase/decrease number of neurons\n",
    "        if is_training and config.keep_prob < 1:\n",
    "            lstm_cell = tf.nn.rnn_cell.DropoutWrapper(lstm_cell,\n",
    "                                                      output_keep_prob=config.keep_prob)\n",
    "        # stacks up cells\n",
    "        if config.num_layers == 0:\n",
    "            print(\"Basic LSTM Cell\")\n",
    "            self._cell = cell = lstm_cell\n",
    "        else:\n",
    "            print(\"Multi RNN Cell of \" + str(config.num_layers))\n",
    "            self._cell = cell = tf.nn.rnn_cell.MultiRNNCell([lstm_cell] * config.num_layers)\n",
    "\n",
    "        self._initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "        # looks up embeddings and gets input\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            embedding = tf.get_variable(\"embedding\", [vocab_size, hidden_size])\n",
    "            inputs = tf.nn.embedding_lookup(embedding, self._input_data)\n",
    "\n",
    "        # calculates dropouts.\n",
    "        if is_training and config.keep_prob < 1:\n",
    "            # the second argument is a probablitiy of keep_prob.\n",
    "            # outputs will be scaled by 1/keep_prob.\n",
    "            inputs = tf.nn.dropout(inputs, config.keep_prob)\n",
    "\n",
    "        state = self._initial_state\n",
    "        with tf.variable_scope(\"RNN\"):\n",
    "            x = [tf.squeeze(input_, [1]) for input_ in tf.split(1, num_steps, inputs)]\n",
    "            # updates a state and computes outputs\n",
    "            outputs, state = rnn.rnn(cell, x, initial_state=state)\n",
    "\n",
    "        output = tf.reshape(tf.concat(1, outputs), [-1, hidden_size])\n",
    "        # softmax of weights\n",
    "        softmax_w = tf.get_variable(\"softmax_w\", [hidden_size, vocab_size])\n",
    "        # softmax of bias\n",
    "        softmax_b = tf.get_variable(\"softmax_b\", [vocab_size])\n",
    "        # caclulates y = Wx + b (score y is called logits)\n",
    "        logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "         # computes softmax cross entropy loss of batch_size\n",
    "        loss = tf.nn.seq2seq.sequence_loss_by_example(\n",
    "            [logits],\n",
    "            [tf.reshape(self._targets, [-1])],\n",
    "            [tf.ones([batch_size * num_steps])])\n",
    "        # gets scalar value of normalized loss\n",
    "        self._cost = cost = tf.reduce_sum(loss) / batch_size\n",
    "        self._final_state = state\n",
    "        self._probs = tf.nn.softmax(logits)\n",
    "\n",
    "        if not is_training:\n",
    "            return\n",
    "\n",
    "        self._lr = tf.Variable(0.0, trainable=False)\n",
    "        tvars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars),\n",
    "                                          config.max_grad_norm)\n",
    "        # optimazation\n",
    "        optimizer = tf.train.GradientDescentOptimizer(self.lr)\n",
    "        self._train_optimizer = optimizer.apply_gradients(zip(grads, tvars))\n",
    "        \n",
    "        # updates embeddings. computes similarity based on cosine distance\n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(embedding), 1, keep_dims=True))\n",
    "        self._normalized_embeddings = embedding / norm\n",
    "\n",
    "    def assign_lr(self, session, lr_value):\n",
    "        session.run(tf.assign(self.lr, lr_value))\n",
    "\n",
    "    @property\n",
    "    def input_data(self):\n",
    "        return self._input_data\n",
    "\n",
    "    @property\n",
    "    def targets(self):\n",
    "        return self._targets\n",
    "\n",
    "    @property\n",
    "    def initial_state(self):\n",
    "        return self._initial_state\n",
    "\n",
    "    @property\n",
    "    def cost(self):\n",
    "        return self._cost\n",
    "\n",
    "    @property\n",
    "    def cell(self):\n",
    "        return self._cell\n",
    "\n",
    "    @property\n",
    "    def final_state(self):\n",
    "        return self._final_state\n",
    "\n",
    "    @property\n",
    "    def probs(self):\n",
    "        return self._probs\n",
    "\n",
    "    @property\n",
    "    def lr(self):\n",
    "        return self._lr\n",
    "\n",
    "    @property\n",
    "    def train_optimizer(self):\n",
    "        return self._train_optimizer\n",
    "    \n",
    "    # sampling function to generate a sequence of words\n",
    "    def sample(self, session, words, vocab, num=200, prime=' ', sampling_type=1):\n",
    "        state = session.run(self.cell.zero_state(1, tf.float32))\n",
    "        if not len(prime) or prime == \" \":\n",
    "            prime  = random.choice(list(vocab.keys()))    \n",
    "        print (prime)\n",
    "        for word in prime.split()[:-1]:\n",
    "            print (word)\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0, 0] = vocab.get(word,0)\n",
    "            feed = {self.input_data: x, self.initial_state: state}\n",
    "            [state] = session.run([self.final_state], feed)\n",
    "         \n",
    "        def weighted_pick(weights):\n",
    "            t = np.cumsum(weights)\n",
    "            s = np.sum(weights)\n",
    "            return(int(np.searchsorted(t, np.random.rand(1)*s)))\n",
    "\n",
    "        ret = prime\n",
    "        word = prime.split()[-1]\n",
    "        for n in range(num):\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0, 0] = vocab.get(word,0)\n",
    "            feed = {self.input_data: x, self.initial_state: state}\n",
    "            [probs, state] = session.run([self.probs, self.final_state], feed)\n",
    "            p = probs[0]\n",
    "\n",
    "            if sampling_type == 0:\n",
    "                sample = np.argmax(p)\n",
    "            else: # sampling_type == 1 default:\n",
    "                sample = weighted_pick(p)\n",
    "\n",
    "            pred = words[sample]\n",
    "            ret += ' ' + pred\n",
    "            word = pred\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Configurations\n",
    "Config class defines configuration parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    init_scale = 0.1       # the initial random range of the weights\n",
    "    learning_rate = 1.0    # the initial value of a learning rate\n",
    "    max_grad_norm = 5      # the maximum permissible norm of the gradient\n",
    "    num_layers = 3         # the number of LSTM layers\n",
    "    num_steps = 20         # the number of unrolled steps of LSTM\n",
    "    hidden_size = 400      # the number of LSTM units (neurons)\n",
    "    forget_bias= 0.5       # the biases of the forget gate\n",
    "    max_epoch = 6          # the number of epochs with the initial learning rate\n",
    "    max_max_epoch = 18     # the total number of epochs\n",
    "    keep_prob = 0.5        # the probability of keeping weights in dropout layer\n",
    "    lr_decay = 0.6         # the learning rate defay factor\n",
    "    batch_size = 20        # the batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Sequence of words\n",
    "The function below generates sequence of words. We can try this part many times. When the sampling type is 1(weight pick), the result varies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x10efb1d50>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi RNN Cell of 3\n",
      "Elizabeth\n",
      "Elizabeth laugh to which any other, that stopt this think?\" \"Oh, \"Yes--no--never young man of sure and I really ever could disapprove for me.\" CHAPTER paused. I can't only be happy to extraordinary on all the name of your pleasures, secret or very unreasonable; everybody am as going off, to confession at both of Miss Woodhouse, it I am afraid suppose; and his modesty, to-morrow of us first--she has the table) which gave a mother of it: her would be reasonably afraid of explain:--there \"Oh!\" I think, not altogether to be out of your side, by She as this, but you know.\" Emma saw his comparison without herself. \"I lives, could another, cannot think on my sister's poor earl's creature!\" said to inquiry, and I knew unlike his continuance in me: not not to have heard himself talking, I would not even happen if your heart to me as assuring you to convince you of Mr. Elton. I understood him at last address.-- \"Yes, she has been half off Carter and expressing his parade to the ladies--married confinement of a income.\" What deplore the unpretending, I refused. Had used all think to see it clear to doubt. The meeting immediately. no,\n"
     ]
    }
   ],
   "source": [
    "save_dir = \"saved\"\n",
    "\n",
    "# num: the number of words to generate\n",
    "# prime: the seed word(s)\n",
    "# sampling_type: sampling type, 0: argmax, 1: weight pick\n",
    "def sample(num, prime, sampling_type):\n",
    "    with open(os.path.join(save_dir, 'words_vocab.pkl'), 'rb') as f:\n",
    "        words, vocab = cPickle.load(f)\n",
    "    sample_config = Config()\n",
    "    sample_config.vocab_size = len(vocab)\n",
    "    sample_config.batch_size=1\n",
    "    sample_config.num_steps =1\n",
    "    initializer = tf.random_uniform_initializer(-sample_config.init_scale,\n",
    "                                                sample_config.init_scale)\n",
    "    with tf.variable_scope(\"model\", reuse=None, initializer=initializer):\n",
    "        model = Model(is_training=True, config=sample_config)\n",
    "    with tf.Session() as session:\n",
    "        tf.initialize_all_variables().run()\n",
    "        saver = tf.train.Saver(tf.all_variables())\n",
    "        ckpt = tf.train.get_checkpoint_state(save_dir)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            saver.restore(session, ckpt.model_checkpoint_path)\n",
    "            print(model.sample(session, words, vocab, num, prime, sampling_type))\n",
    "            \n",
    "sample(200, \"Elizabeth\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
